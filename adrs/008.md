---
id: "0008"
title: Mobile Offline Sync Strategy
status: proposed
date: 2025-10-18
---

## Context and Problem Statement

Field Operatives at MobilityCorp frequently operate in areas with **weak or intermittent connectivity** —  
underground parking structures, suburban zones, or remote service areas.

To maintain operational continuity and data integrity, the **Field-Ops App** must:

- Function seamlessly **offline** (view assigned routes, record actions, update task status).  
- **Queue updates locally** until network connectivity returns.  
- Perform **safe, deterministic synchronisation** once reconnected.  
- Resolve conflicts predictably when the same task or record is modified on multiple devices.  
- Maintain **auditability** (who did what, when) for compliance and SLA tracking.  

The goal is to guarantee **reliability and user trust** while maintaining **data integrity and explainability** for operational analytics.

---

## Questions

- Should the mobile app store a full local database or a lightweight cache?  
- What is the conflict-resolution model — **client-wins**, **server-wins**, or **merge-policy**?  
- How can we ensure **idempotency** and **audit trail continuity** across sync cycles?  
- What frameworks or storage layers best support offline-first architectures?  
- How do we handle **schema evolution** and backward compatibility for local data?  

---

## Options

### Option A — Online-Only
App requires constant connectivity; no local persistence.

**Pros**
- Simplest implementation.  
- Centralised logic; fewer sync concerns.

**Cons**
- Unusable in connectivity gaps.  
- Data loss risk; poor user experience.  

---

### Option B — Full Local Cache with Periodic Sync
Entire task/route dataset cached locally; bi-directional sync on reconnect.

**Pros**
- Robust offline experience.  
- Supports complete audit of local actions.

**Cons**
- Large local footprint; more complex conflict handling.  
- Requires schema migrations on app updates.

---

### Option C — Local Event Queue (Recommended)
App stores a **local queue of user actions/events** (start task, complete swap, log issue).  
Each event has a **UUID**, **timestamp**, and **hash** for idempotency and replay safety.  
Data required for UX (routes, tasks, maps) is cached separately.  
A **sync engine** replays queued events to the backend when online, then reconciles new server state.

**Pros**
- Lightweight; minimal local storage.  
- Predictable idempotent event flow.  
- Natural fit for MobilityCorp’s event-driven backend (Kafka topics).  
- Simplifies schema evolution (event contracts stable over time).  

**Cons**
- Requires robust conflict and retry handling.  
- Slightly more engineering upfront for the local event engine.  

---

### Option D — CRDT-based Data Replication
Use Conflict-Free Replicated Data Types (e.g., Automerge, Realm Sync) for eventual consistency.

**Pros**
- Elegant mathematical conflict resolution.  
- Ideal for true collaborative editing scenarios.  

**Cons**
- Overkill for operational workflows.  
- Higher compute and battery cost on mobile.  
- Harder to audit and replay deterministically.  

---

## Recommendation

Adopt **Option C – Local Event Queue with Server-Authoritative Reconciliation**.

- The **Field-Ops App** maintains a persistent, append-only queue of user actions (task status changes, notes, battery swap logs).  
- Each event is **idempotent**, **timestamped**, and carries a **local operation hash** to detect duplicates.  
- The **Mobile Sync Service** (middleware) processes queued events in order and returns updated state snapshots.  
- Conflicts are resolved by **server-wins + merge heuristics**:
  - If a task is already completed in server state → reject duplicate.  
  - If timestamps differ → keep newest; append audit record.  
- Local caches of routes/tasks are versioned via **ETags** or **revision numbers**.  
- On reconnect, app performs **3-phase sync**:  
  1. Push queued events.  
  2. Fetch updated routes/tasks.  
  3. Reconcile and notify user.  
- Sync retries use **exponential backoff** with power-aware scheduling.

---

## Consequences

**Positive**
- Reliable field operations even without connectivity.  
- Seamless sync across thousands of operatives.  
- Deterministic replay for audit and reconciliation.  
- Aligns with event-driven backend (Kafka topics).  

**Trade-offs**
- Slightly more client complexity (local store + sync queue).  
- Requires careful handling of merge conflicts and retries.  
- Additional testing for network edge cases and app restarts.  

---

## Implementation Details (High-level)

| Component | Responsibility | Notes |
|---|---|---|
| **Field-Ops App** | Local event queue, caching, offline UX | Uses SQLite/Realm; encrypt local store |
| **Sync Engine (client-side)** | Detect connectivity, trigger push/pull sync, exponential backoff | Background service; battery-aware |
| **Mobile Sync API (backend)** | Receives events, deduplicates, applies to canonical DB, returns updated state | Publishes successful events to Kafka |
| **Conflict Resolver** | Applies server-wins with audit logging | Logs discrepancies for BI & QA |
| **Audit Log** | Stores all event hashes and reconciliation outcomes | Immutable for compliance |
| **Notification Service** | Notifies app of new assignments or plan updates | WebSocket/push fallback to polling |

**Data Model Example**
```text
{
  "event_id": "uuid",
  "task_id": "task-1234",
  "user_id": "op-5678",
  "timestamp": "2025-10-18T09:30:00Z",
  "operation": "battery_swap_complete",
  "payload": {...},
  "hash": "sha256(event_id+payload)",
  "retries": 0
}
```

<img width="3656" height="1422" alt="008" src="https://github.com/user-attachments/assets/181e64bb-99a0-4a7c-a33c-bd138c74753c" />

---

### Risks and Mitigations

| Risk                                         | Likelihood | Impact | Mitigation                                              |
| -------------------------------------------- | ---------: | -----: | ------------------------------------------------------- |
| Event loss or duplication                    |        Low |   High | UUID + hash; idempotent server processing; retries      |
| Clock skew causing mis-ordering              |        Med |    Med | Use server timestamp on commit; sync NTP periodically   |
| Storage exhaustion on device                 |    Low–Med |    Med | Compact queue; cap event retention (e.g., 30 days)      |
| Data corruption (app crash mid-sync)         |        Med |    Med | Write-ahead logging; transactional local DB             |
| Unauthorized local access                    |        Low |   High | Encrypt local DB; enforce app-level auth tokens         |
| Inconsistent server state after partial sync |        Med |    Med | Atomic server apply; rollback on failure                |
| User confusion post-sync                     |        Low |    Med | Clear in-app notifications of merges/conflicts          |
| Schema evolution / app version drift         |        Med |    Med | Versioned event contracts; backward compatibility layer |

---

### Alternatives Considered

* CRDT replication – rejected as unnecessary complexity for operational workflows.
* Web-first app – rejected due to offline requirements.
* Manual paper fallback – existing ops burden; replaced by digital queue.


---

## Rationale

Option C (Local Event Queue) is chosen for its alignment with MobilityCorp’s event-driven backend, lightweight local storage, and predictable, idempotent event flow. This approach minimizes local data complexity, supports robust offline operation, and simplifies schema evolution compared to full local database or CRDT-based replication. It also enables strong auditability and compliance, which are critical for operational analytics and regulatory requirements.

## Consequences

**Positive Consequences:**
- Reliable field operations and user trust, even in poor connectivity.
- Deterministic, auditable replay of all user actions for compliance and analytics.
- Seamless integration with event-driven backend (Kafka topics).
- Simplified schema evolution and backward compatibility.

**Negative Consequences / Trade-offs:**
- Increased client complexity (local event queue, sync engine, conflict handling).
- Requires careful merge conflict and retry logic.
- Additional testing for edge cases (network, app restarts, schema drift).

## Stakeholders

- Field Operations: End users; rely on app for daily tasks and reporting.
- Mobile Engineering: Responsible for app development, offline UX, and sync engine.
- Backend Engineering: Owns Mobile Sync API, conflict resolver, and audit log integration.
- Compliance & Legal: Oversee auditability, data privacy, and regulatory adherence.
- Product Management: Defines requirements, prioritizes features, and coordinates rollout.

## Reliability, Audit & Compliance

- All local events and sync operations must be idempotent and logged for auditability.
- Sync engine must ensure at-least-once delivery and handle retries, failures, and partial syncs.
- All event hashes, reconciliation outcomes, and conflict resolutions are logged to the immutable Audit Log ([ADR-013](./013.md)).
- Compliance team reviews audit logs quarterly and after any major incident.

## GDPR & Data Privacy Considerations

- Local data (including event queue and caches) must be encrypted at rest and in transit.
- Only the minimum required PII is stored locally, and is purged after successful sync or as per retention policy.
- Users must be able to request data export, correction, or deletion via the app; all such actions are logged.
- Regular privacy impact assessments (PIAs) are conducted by Compliance & Legal.

## Incident Response & Escalation

- Critical sync failures, data corruption, or unauthorized access must trigger automated alerts to Mobile Engineering and Compliance.
- Incident response runbooks must be maintained, including triage, rollback, and user notification procedures.
- Major incidents require post-mortem review and communication to all stakeholders.
- Escalation: Mobile Engineering → Backend Engineering → Compliance/Legal → Executive Leadership.

## Schema Evolution Feedback Loop

- All event contract/schema changes must be proposed via change request, reviewed by Mobile and Backend Engineering, and validated by QA before deployment.
- Breaking changes require migration plan, deprecation notice, and user communication.
- Feedback from field users and downstream analytics must be solicited and documented for each schema change.

## Stakeholder Communication Plan

- Major changes, schema updates, and incident reports are communicated via project meetings, email, and internal documentation.
- Stakeholder roles and responsibilities are reviewed quarterly.
- Feedback channels (in-app, email, ticketing) are monitored by Product and Engineering.

## Risk Ownership & Audit

- Each risk in the Risks and Mitigations table is assigned an owner:
  - Event loss/duplication: Backend Engineering
  - Clock skew: Backend Engineering
  - Storage exhaustion: Mobile Engineering
  - Data corruption: Mobile Engineering
  - Unauthorized access: Mobile Engineering & Compliance
  - Inconsistent server state: Backend Engineering
  - User confusion: Product Management
  - Schema evolution/app drift: Mobile & Backend Engineering
- Risk register is reviewed quarterly or after major incidents.
- Audit logs are owned by Compliance, with technical implementation by Backend Engineering.

## Technology Rationale

- **Local Event Queue**: Lightweight, append-only, supports idempotency and replay.
- **SQLite/Realm**: Mature, reliable local storage with encryption support.
- **Mobile Sync API**: Enables server-authoritative reconciliation and audit logging.
- **Kafka**: Backend event streaming, aligns with MobilityCorp’s architecture.
- **Audit Log**: Immutable, tamper-evident record for compliance.

## Follow-ups / Open Issues

- Finalize event contract schemas and sync API specifications.
- Define SLAs for sync latency, reliability, and auditability.
- Review and update compliance documentation as regulations evolve.
- Confirm stakeholder sign-off at each rollout milestone.
- Track open issues with device storage, schema migration, and user feedback.
- Document and test incident response and escalation procedures.

---

## Related ADRs

- [ADR-002 – External Dependency SLA & Retry Handling](./002.md)
- [ADR-005 – Routing Solver (VRP-TW)](./005.md)
- [ADR-006 – Alerts & Theft Detection Logic](./006.md)
- [ADR-007 – Data Platform Architecture](./007.md)
- [ADR-013 – Immutable Audit Log Architecture](./013.md)
- [ADR-016 – AI Interchange Layer](./016.md)

---

[Back to ADR Index](../README.md)
