---
id: "007"
title: Data Platform Architecture
status: proposed
date: 2025-10-22
---


## Context and Problem Statement

MobilityCorp, as a multi-region ride sharing and fleet operations platform, operates across multiple European countries and relies on a steady stream of **telemetry, routing, weather, and transport data** to drive AI services such as [routing (ADR-005)](./005.md), [demand prediction (ADR-004)](./004.md), and [concierge personalisation (ADR-009)](./009.md).

Earlier prototypes used a UK-centric (TfL) feed model and static CSV ingest. To scale and maintain compliance, MobilityCorp requires a **region-agnostic, event-driven data platform** that unifies diverse feeds into a single, EU-resident analytical and operational backbone. This ADR builds on the event-driven, reliable integration patterns established in [ADR-001](./001.md), [ADR-002](./002.md), and [ADR-011](./011.md), ensuring that data ingestion, transformation, and delivery are robust, auditable, and seamlessly integrated with upstream and downstream operational systems.

---


### Key Questions

- How can regional mobility data (e.g., RATP, BVG, ATAC) be standardised into one schema?  
- How should we incorporate weather and environmental context into real-time pipelines?  
- What governance mechanisms ensure schema evolution and data lineage across regions?  
- How do AI models consume these streams reliably and reproducibly?  
- How do we balance cost, latency, and audit requirements?

---


## Options

### Option A — Central Batch ETL
Nightly jobs pull raw data from all sources into a warehouse.

**Pros**
- Simple to implement.  
**Cons**
- High latency, no real-time response, limited lineage and schema governance.

---


### Option B — Event-Driven Lakehouse Architecture (Recommended)
Use **Kafka** for streaming ingestion and **Databricks Delta Lake** for unified batch and stream processing.

**Pros**
- Real-time, region-aware data availability.
- Natural fit for MobilityCorp’s AI and routing workloads.
- Schema evolution and audit support via Delta metadata.

**Cons**
- Increased operational complexity.
- Requires strict schema governance and topic management.

---


## Decision

We will adopt **Option B – Event-Driven Lakehouse Architecture** for MobilityCorp’s data platform, with region and weather enrichment, robust schema governance, and EU data residency. This approach aligns with the platform’s commitment to operational excellence, compliance, and architectural consistency across domains.

**Rationale:**
- Real-time, region-aware data pipelines are essential for supporting AI-driven operations, compliance, and customer experience across Europe.
- Integration with Kafka and Databricks Delta Lake enables unified batch and stream processing, supporting both operational and analytical workloads.
- Schema registry, audit log ([ADR-011](./011.md)), and observability ([ADR-013](./013.md)) ensure strong data lineage, compliance, and operational transparency.
- While this approach introduces infrastructure and governance complexity, the benefits in scalability, compliance, and real-time analytics outweigh the trade-offs for a multi-region mobility platform.

Option A (batch ETL only) was rejected due to lack of timeliness, lineage, and governance required for real-time, multi-region operations.

**Key decisions:**
1. **Kafka Event Bus:** backbone for ingesting telemetry, transport, weather, and demand events.
2. **Transport Adapter Service:** plug-in connectors per country normalising GTFS-RT or proprietary feeds into `transport.status` events.
3. **Weather Collector:** integrates external weather APIs; publishes `weather.conditions` to Kafka.
4. **Databricks Structured Streaming:** cleans, joins, and enriches events into Delta tables.
5. **Data Residency:** all clusters, Kafka topics, and Delta storage remain within the EU.
6. **Schema Registry:** all topics version-controlled with compatibility checks.
7. **Downstream Consumers:**
   - [Feature Store (ADR-016)](./016.md)
   - [Routing Solver (ADR-005)](./005.md)
   - [Demand Prediction (ADR-004)](./004.md)
   - [Concierge Service (ADR-009)](./009.md)
8. **Audit & Observability:** ingestion lineage logged to [ADR-011](./011.md); metrics published to [ADR-013](./013.md).

---


## Consequences

**Positive Consequences:**
- Region-aware, real-time pipelines supporting AI and operational workloads.
- Unified transport and weather context for downstream services and models.
- Strong data lineage, auditability, and EU compliance.

**Negative Consequences / Trade-offs:**
- More infrastructure and governance overhead.
- Higher complexity in multi-region schema versioning and topic management.

**Stakeholders:**
- Platform Engineering: responsible for data platform development, integration, and maintenance.
- Data Engineering: responsible for connectors, pipelines, and schema governance.
- AI/ML Engineering: define data requirements, validate data quality, and consume enriched streams.
- Compliance and Legal: oversee data residency, audit, and regulatory requirements.

**Reliability and Integration:**
- All data ingestion, transformation, and delivery must use reliable, idempotent delivery patterns as established in [ADR-001](./001.md) (e.g., Outbox + Worker Queue for Kafka topics).
- Downstream consumers must be able to handle retries, late or missing events, and ensure at-least-once processing semantics.

**Audit and Compliance:**
- All pipeline events, schema changes, and data lineage must be logged to the Immutable Audit Log ([ADR-011](./011.md)) and be accessible for compliance review.
- Regular audits and SLA reviews must be scheduled and owned by Platform Engineering and Compliance teams. Audit frequency: quarterly, or after any major incident. Compliance team is responsible for regulatory monitoring and escalation.

**Implementation and Rollout Plan:**
1. Develop and document canonical event schemas for all data platform topics.
2. Build and validate regional transport adapters and weather collectors.
3. Integrate with downstream consumers ([ADR-004](./004.md), [ADR-005](./005.md), [ADR-009](./009.md), [ADR-016](./016.md)) and ensure reliable event delivery.
4. Register all schemas and log all pipeline events to the Audit Log ([ADR-011](./011.md)).
5. Establish monitoring dashboards and alerting for pipeline health, latency, and event delivery failures.
6. Communicate rollout plan, milestones, and responsibilities to all stakeholders via regular project updates and review meetings.


**Follow-ups / Open Issues:**
- Finalize event schemas and data contracts with all consuming and producing services.
- Define SLAs for data latency, reliability, and auditability.
- Review and update compliance documentation as regulations evolve (see GDPR/Data Privacy section).
- Confirm stakeholder sign-off at each rollout milestone.
- Track open integration issues with regional data providers and update risk register as needed.
- Establish feedback loop for schema evolution: all schema changes must be reviewed by Data Engineering, validated by downstream consumers, and communicated to all stakeholders before deployment. Breaking changes require a deprecation notice and migration plan.
- Document and test incident response and escalation procedures for data pipeline failures, data quality issues, and compliance breaches (see Incident Response & Escalation section).
- Monitor and address open issues related to cross-region data residency, latency, and provider API changes.


## GDPR & Data Privacy Considerations

- All data platform components must enforce EU data residency. No data (including logs, backups, or audit trails) may leave the EU region.
- Personally Identifiable Information (PII) must be minimized, encrypted at rest and in transit, and processed only as required for operational or analytical purposes.
- Data subject rights (access, erasure, rectification) must be supported via platform APIs and logged in the Audit Log ([ADR-011](./011.md)).
- Regular privacy impact assessments (PIAs) must be conducted and reviewed by Compliance and Legal teams.
- Data retention and deletion policies must be documented and enforced for all data stores and pipelines.

## Incident Response & Escalation

- All critical pipeline failures, data quality issues, or compliance breaches must trigger automated alerts to Platform Engineering and Compliance teams.
- Incident response runbooks must be maintained, including steps for triage, root cause analysis, and communication to affected stakeholders.
- Major incidents require a post-mortem review, with action items tracked to closure and summary published to all stakeholders.
- Escalation paths: Platform Engineering → Data Engineering → Compliance/Legal → Executive Leadership, depending on severity and regulatory impact.

## Schema Evolution Feedback Loop

- All schema changes must be proposed via a formal change request, reviewed by Data Engineering, and validated by downstream consumers before approval.
- Schema Registry enforces compatibility checks; breaking changes require a migration plan and deprecation notice.
- Feedback from downstream consumers (AI/ML, Feature Store, Routing Solver, etc.) must be solicited and documented for each schema change.

## Stakeholder Communication Plan

- Major architectural changes, schema updates, and incident reports must be communicated to all stakeholders via regular project meetings, email updates, and documentation in the internal knowledge base.
- Stakeholder roles and responsibilities are documented in this ADR and must be reviewed quarterly.
- Feedback channels (e.g., Slack, email, ticketing system) must be monitored and actioned by Platform Engineering.

## Risk Ownership & Audit

- Each risk identified in the Risks and Mitigations table is assigned an owner:
   - Provider API schema drift: Data Engineering
   - Weather API downtime: Platform Engineering
   - Cross-region latency: Platform Engineering
   - High data volume costs: Platform Engineering (with Finance)
   - Regulatory non-compliance: Compliance and Legal
- Risk register must be reviewed and updated quarterly, or after any major incident.
- Audit logs and compliance reviews are owned by Compliance and Legal, with Platform Engineering responsible for technical implementation.

## Technology Rationale

- **Kafka**: Chosen for its maturity, strong ecosystem, and support for high-throughput, low-latency event streaming across regions. Enables reliable ingestion and decoupling of producers/consumers.
- **Databricks Delta Lake**: Provides unified batch and streaming analytics, ACID transactions, and schema evolution support. Well-integrated with Spark and scalable for MobilityCorp’s needs.
- **Schema Registry**: Ensures schema governance, compatibility, and traceability for all event topics.
- **Audit Log**: Immutable, tamper-evident record of all pipeline and schema events, supporting compliance and incident investigation.
- **Observability/OTel**: Unified metrics and tracing for proactive monitoring and troubleshooting.

---

## Implementation Details (High-level)

| Component | Responsibility | Notes |
|---|---|---|
| **Kafka (EU Cluster)** | Event backbone for all real-time feeds | Confluent/MSK EU region |
| **Transport Adapter Layer** | Connect to local transport APIs, normalise schema | Emits `transport.status` |
| **Weather Collector** | Pull weather data periodically from external APIs | Emits `weather.conditions` |
| **Databricks Pipelines** | Transform, join, and enrich streams | Writes curated Delta tables |
| **Schema Registry** | Manage topic contracts and evolution | Enforced in CI/CD |
| **Audit Log ([ADR-011](./011.md))** | Record pipeline and schema events | Tamper-evident ledger |
| **Observability ([ADR-013](./013.md))** | Monitor latency, volume, and errors | Unified OTel metrics |
| **Feature Store ([ADR-016](./016.md))** | Consume enriched data for AI training/inference | Feeds downstream models |
| **Routing Solver ([ADR-005](./005.md))** | Subscribe to enriched transport/weather events | Context for route optimisation |

**Sequence Example**

<img width="3554" height="978" alt="007" src="https://github.com/user-attachments/assets/ce8034d8-dff4-433a-83fc-356850eaeb98" />

---

## Risks and Mitigations

| Risk                      | Likelihood | Impact | Mitigation                            |
| ------------------------- | ---------: | -----: | ------------------------------------- |
| Provider API schema drift |        Med |   High | Schema registry & contract tests      |
| Weather API downtime      |        Low |    Med | Caching, retry logic                  |
| Cross-region latency      |        Med |    Med | Regional ingestion nodes              |
| High data volume costs    |        Med |    Med | Tiered retention and aggregation      |
| Regulatory non-compliance |        Low |   High | EU region enforcement and audit hooks |

---

## Alternatives Considered

* Batch ETL only – rejected for lack of timeliness.
* Event-driven ingestion – chosen for scalability, governance, and real-time analytics alignment.


## Related ADRs

- [ADR-001 – External Dependency SLA & Retry Handling](./001.md)
- [ADR-002 – Pricing & Billing Microservice Architecture](./002.md)
- [ADR-004 – Demand Prediction Model](./004.md)
- [ADR-005 – Routing Solver (VRP-TW)](./005.md)
- [ADR-009 – Concierge / Personalisation Service Architecture](./009.md)
- [ADR-011 – Immutable Audit Log Architecture](./011.md)
- [ADR-013 – Observability & Metrics Standardisation](./013.md)
- [ADR-016 – AI Data Pipeline & Feature Store Design](./016.md)

---

[Back to ADR Index](../README.md)
