---
id: "0007"
title: Data Platform Architecture
status: proposed
date: 2025-10-21
---

## Context and Problem Statement

MobilityCorp’s AI and analytics capabilities rely on a steady flow of operational data — vehicle telemetry, routing decisions, trip outcomes, and user interactions.  
This data must be ingested, stored, and made accessible for:

- **Analytics and reporting** (performance, utilisation, SLA tracking).  
- **Model training** (forecasting, optimisation, personalisation).  
- **AI inference pipelines** (real-time and batch).  

The platform must comply with **EU data residency**, ensure **data quality and lineage**, and remain **extensible** for future use cases such as **semantic retrieval (Vector DB)** and **AI explainability**.

---

## Questions

- Should ingestion remain batch-based or evolve to event-driven?  
- How do we balance near real-time analytics with compliance and cost?  
- Where should long-term storage and model feature generation occur?  
- Should we introduce a retrieval layer (e.g., Vector DB) alongside structured data?  
- How do we ensure privacy boundaries and GDPR alignment across all data flows?

---

## Options

### Option A — Centralised Relational Warehouse
Aggregate all mobility data into a single SQL-based warehouse (e.g., PostgreSQL or Snowflake).

**Pros**
- Simple architecture.  
- Mature tooling and governance.  

**Cons**
- Poor fit for streaming data and event-driven architectures.  
- Limited scalability for AI and ML training workloads.  

---

### Option B — Data Lake with Batch ETL
Store data in object storage (e.g., S3, Azure Blob) with nightly ETL into Databricks or similar.

**Pros**
- Scalable; cost-effective for large datasets.  
- Works well with analytical tools.  

**Cons**
- High latency; slow for near real-time use cases.  
- Complex transformations and dependency management.  

---

### Option C — Event-Driven Lakehouse (Recommended)
Adopt an **event-driven lakehouse architecture** with **Kafka ingestion** and **Databricks** as the analytical and ML backbone.  
Augment with an **optional semantic retrieval layer (Vector DB)** for future AI-driven use cases (e.g., personalisation, contextual retrieval, explainability).

**Pros**
- Real-time ingestion and processing.  
- Unified data model for analytics and ML.  
- Extensible for AI and semantic workloads.  
- Compliant with EU data residency policies.  

**Cons**
- Requires additional orchestration (streaming, schema evolution).  
- Higher initial setup and monitoring overhead.  

---

## Recommendation

Adopt **Option C – Event-Driven Lakehouse Architecture** with Databricks as the core data warehouse and Kafka for ingestion.  
Reserve capacity for an **optional Vector Database** (e.g., Weaviate, Pinecone, or Databricks Vector Search) to support retrieval-augmented AI services such as the **Personalisation / Concierge Service** (ADR-009).

Key elements:

1. **Kafka Ingestion Layer** — All operational systems publish events (telemetry, routing, user actions).  
2. **Databricks Lakehouse** — Central analytical environment for storage, transformation, model training, and BI.  
3. **Feature Store** — Curated dataset for AI model training and inference.  
4. **Vector Database (optional)** — Embedding-based semantic retrieval for contextual AI.  
5. **Governance Layer** — Access control, lineage, data quality, and GDPR compliance enforcement.

---

## Consequences

**Positive**
- Real-time data availability for analytics and AI.  
- Unified storage and compute pattern.  
- Extensible to support vector-based retrieval and explainability.  
- Aligns with MobilityCorp’s privacy and compliance strategy.

**Trade-offs**
- More complex to operate than traditional warehouse.  
- Requires schema management and strong observability for event streams.  
- Additional cost for optional retrieval infrastructure.

---

## Implementation Details (High-level)

| Component | Responsibility | Notes |
|---|---|---|
| **Kafka / Event Bus** | Ingest trip, user, and telemetry events in real time | Schema Registry for contract validation |
| **Databricks Lakehouse** | Store, transform, and serve analytical data | EU-hosted; Delta tables for ACID transactions |
| **Feature Store** | Expose curated datasets for ML models | Reused by prediction, routing, and personalisation services |
| **Vector Database (optional)** | Store embeddings for semantic retrieval | Supports contextual AI (see ADR-009) |
| **Access & Governance Layer** | Data catalogue, lineage, role-based access | Integrates with enterprise IAM |
| **Monitoring & Audit** | Data quality checks, drift detection, compliance reporting | Automated alerts and lineage dashboards |

**Sequence Example**

<img width="4528" height="1252" alt="007" src="https://github.com/user-attachments/assets/f067aced-48ca-4453-ac60-bb61e8e32227" />

---

## Risks and Mitigations

| Risk                            | Likelihood | Impact | Mitigation                                    |
| ------------------------------- | ---------: | -----: | --------------------------------------------- |
| Kafka schema drift              |        Med |   High | Use Schema Registry; enforce contracts        |
| Warehouse latency or downtime   |        Low |   High | Tiered storage and auto-scaling               |
| Data duplication across systems |        Med |    Med | Deduplication pipelines with unique event IDs |
| GDPR non-compliance             |        Low |   High | EU data residency, anonymisation, audit logs  |
| Vector DB sprawl or cost creep  |        Low |    Med | Keep optional until justified by AI workloads |
| Event loss in transit           |        Low |   High | Enable Kafka replication and retries          |

---

## Alternatives Considered

* Centralised relational warehouse — rejected for lack of scalability and streaming support.
* Batch lake + nightly ETL — rejected for latency and operational complexity.
* Hybrid event-driven lakehouse — chosen for extensibility, performance, and AI-readiness.

## Links

* ADR-002 – External Dependency SLA & Retry Handling
* ADR-005 – Routing Solver (VRP-TW)
* ADR-006 – Alerts & Theft Detection Logic
* ADR-008 – Mobile Offline Sync Strategy
* ADR-009 – Personalisation / Concierge Service Architecture
* ADR-013 – Immutable Audit Log Architecture
* ADR-016 – AI Interchange Layer
