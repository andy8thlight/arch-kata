---
id: "0016"
title: AI Data Pipeline & Feature Store Design
status: proposed
date: 2025-10-22
---

## Context and Problem Statement

MobilityCorp’s AI ecosystem (forecasting, routing solver, anomaly detection, concierge service) depends on high-quality, reproducible, and auditable **features** derived from operational telemetry.  
Currently, model training pipelines are fragmented — each team builds its own ETL and stores derived features separately.  
This causes duplication, inconsistent feature definitions, and difficulty in reproducing model behaviour for compliance or retraining.

A unified **AI Data Pipeline and Feature Store** will ensure:
- **Consistent feature definitions** across models and teams.  
- **End-to-end lineage** from raw telemetry → feature → model input → inference output.  
- **EU data residency and governance compliance.**  
- **Reproducibility** for audits, debugging, and retraining.  
- **Tight integration** with the Data Platform (ADR-007), Audit Log (ADR-011), and Observability stack (ADR-013).  

---

## Questions

- Should features be materialised centrally (shared store) or built dynamically per model?  
- How do we ensure schema consistency across AI domains (routing, demand, anomaly)?  
- What is the right cadence for feature refresh (streaming vs batch)?  
- How do we version features and maintain lineage from telemetry to model?  
- How will this integrate with Databricks pipelines, Kafka streams, and the AI Interchange (ADR-014)?  

---

## Options

### Option A — Ad-hoc Feature Engineering (Status Quo)
Each team builds its own data prep pipeline and stores features in local datasets.

**Pros**
- High flexibility per use case.  
- No central coordination required.

**Cons**
- Duplicated logic and inconsistent semantics.  
- Difficult to reproduce models or audit feature lineage.  
- Slower model iteration and higher maintenance cost.

---

### Option B — Batch-Only Central Feature Repository
A nightly job computes and stores features in a shared warehouse.

**Pros**
- Central control and documentation.  
- Easy to audit and trace.  

**Cons**
- Not suitable for real-time inference.  
- Stale features for time-sensitive models (e.g., routing).  

---

### Option C — Unified Real-Time + Batch Feature Store (Recommended)
Adopt a **central Feature Store** built on the existing **Databricks Lakehouse** and **Kafka Event Bus (ADR-007)**, supporting both batch and streaming ingestion.  
Each feature is registered, versioned, and documented, with lineage tracking and audit integration.

**Pros**
- Real-time and offline parity (same feature definitions for training and inference).  
- Central schema and lineage; reproducible experiments.  
- Seamless integration with Databricks, AI Interchange (ADR-014), and SageMaker (ADR-015).  
- Automatic logging to the Immutable Audit Log (ADR-011).  

**Cons**
- Requires initial governance overhead.  
- Slightly more complexity in metadata management.  

---

## Recommendation

Adopt **Option C – Unified Real-Time + Batch Feature Store** as the core AI data backbone.

**Key decisions**
1. **Source Telemetry:** All raw events (routing, tasks, vehicle telemetry) ingested via **Kafka (ADR-007)**.  
2. **Transformation Layer:** Use Databricks structured streaming and Delta pipelines for cleaning, joining, and aggregating telemetry.  
3. **Feature Registry:** Central registry defines feature schemas, owners, freshness, and versions.  
4. **Materialisation:**  
   - Batch features → stored in Delta tables.  
   - Real-time features → served from Redis / online store API.  
5. **Lineage & Audit:** Each feature record linked to raw source and logged in **Audit Log (ADR-011)**.  
6. **Integration:**  
   - **Model Training (ADR-015)** reads from offline store.  
   - **AI Interchange (ADR-014)** pulls real-time features for inference.  
7. **Observability:** Metrics for data freshness, feature drift, and pipeline latency emitted under ADR-013 schema.

---

## Consequences

**Positive**
- Reproducible model training and explainability.  
- Streamlined cross-team collaboration via shared feature definitions.  
- Real-time and batch feature parity reduces deployment bugs.  
- Simplifies monitoring of drift and data quality.  

**Trade-offs**
- Increased governance and schema management workload.  
- Requires coordination between Data Engineering and ML Ops teams.  
- Initial migration of legacy pipelines to the unified schema.  

---

## Implementation Details (High-level)

| Component | Responsibility | Notes |
|---|---|---|
| **Kafka / Event Bus** | Ingest raw telemetry and operational data | Shared ingestion layer (ADR-007) |
| **Databricks Pipelines** | Transform telemetry into structured features | Streaming + batch modes |
| **Feature Registry** | Metadata, ownership, and versioning of features | Backed by Delta or Hive metastore |
| **Offline Store (Delta Lake)** | Persistent historical features for training | EU-resident; reproducible snapshots |
| **Online Store (Redis / API)** | Serve real-time features for inference | Cached from latest Kafka streams |
| **AI Interchange Layer (ADR-014)** | Consume features at inference time | Aligns schema between training/inference |
| **Audit Log (ADR-011)** | Immutable log of feature creation and updates | Hash-chained integrity |
| **Observability Stack (ADR-013)** | Monitor freshness, drift, and pipeline latency | OTel-compliant metrics |
| **Model Hosting (ADR-015)** | Consume registered features for inference | Via offline or real-time connectors |

**Sequence Example**


---

## Risks and Mitigations

| Risk                                          | Likelihood | Impact | Mitigation                                           |
| --------------------------------------------- | ---------: | -----: | ---------------------------------------------------- |
| Inconsistent feature definitions across teams |        Med |   High | Enforced schema and governance board                 |
| Data drift or stale features                  |        Med |   High | Freshness monitoring and drift detection alerts      |
| High latency in streaming pipelines           |        Med |    Med | Autoscaling clusters and micro-batch tuning          |
| Feature store downtime impacts inference      |        Low |   High | Cache fallback and replay from Kafka                 |
| Schema versioning conflicts                   |        Med |    Med | Versioned registry and backward-compatible contracts |
| Data residency violations                     |        Low |   High | EU region restriction and encrypted storage          |
| Cost creep due to redundant features          |        Med |    Med | Periodic feature usage audits                        |

---

## Alternatives Considered

* Ad-hoc pipelines per model – rejected for reproducibility issues.
* Batch-only central store – rejected for latency and stale data.
* Unified streaming + batch feature store – chosen for reproducibility, compliance, and cross-model reuse.

## Links

* ADR-007 – Data Platform Architecture
* ADR-011 – Immutable Audit Log Architecture
* ADR-013 – Observability & Metrics Standardisation
* ADR-014 – AI Interchange Layer Architecture
* ADR-015 – Model Hosting & Inference Strategy
* ADR-005 – Routing Solver (VRP-TW)
* ADR-006 – Alerts & Theft Detection Logic
