---
id: "0016"
title: AI Data Pipeline & Feature Store Design
status: proposed
date: 2025-10-22
---

## Context and Problem Statement

MobilityCorp’s AI systems—**Routing Solver (ADR-005)**, **Demand Prediction (ADR-004)**, and **Concierge Service (ADR-009)**—all rely on a consistent, explainable, and region-aware foundation of **features** derived from operational telemetry, transport data, and weather conditions.

The AI Data Pipeline must support:

- **Multi-region ingestion** (localised transport and traffic feeds).  
- **Weather-aware features** (temperature, precipitation, strikes).  
- **Cross-model reuse** (routing, prediction, personalisation).  
- **Full lineage and audit compliance** (ADR-011).  

---

## Questions

- How can regional transport and weather events be unified into shared feature definitions?  
- How do we guarantee schema consistency across routing, prediction, and concierge models?  
- Should features be refreshed continuously (streaming) or in batches?  
- How do we ensure reproducibility for retraining and regulatory audits?  

---

## Options

### Option A — Independent Pipelines Per Model
Each AI model defines its own data pipeline and feature extraction.

**Pros**
- Decentralised flexibility.  
**Cons**
- Redundant logic, inconsistent schemas, poor reproducibility.  

---

### Option B — Central Feature Store with Batch-Only Refresh
Single store populated nightly from data warehouse.

**Pros**
- Central control and versioning.  
**Cons**
- Stale data for real-time inference.  
- Incompatible with dynamic routing or concierge recommendations.  

---

### Option C — Unified Real-Time + Batch Feature Store (Recommended)
Use **Kafka** + **Databricks Delta Lake** to generate and serve consistent feature sets for all AI models, combining real-time (weather, transport, demand) and offline (historical) data.

**Pros**
- Reusable across models (Routing, Prediction, Concierge).  
- Region- and weather-aware features.  
- Fully auditable and reproducible.  
**Cons**
- Requires strong governance and metadata management.  

---

## Recommendation

Adopt **Option C – Unified Real-Time + Batch Feature Store** integrated with the **Data Platform (ADR-007)** and serving both **training** and **inference**.

**Key decisions**
1. **Multi-Source Ingestion:** Transport, weather, telemetry, and demand events from Kafka (ADR-007).  
2. **Transformation Pipelines:** Databricks Structured Streaming aggregates, enriches, and validates features.  
3. **Feature Registry:** central metadata catalog with freshness, version, and region scope (`region_code`).  
4. **Offline Store (Delta Lake):** batch features for model training and validation.  
5. **Online Store (Redis/API):** live features for inference used by AI Interchange (ADR-014).  
6. **Feature Groups:**  
   - `routing_features` → ETA, congestion, depot capacity.  
   - `demand_features` → past usage, holidays, weather, strike indicators.  
   - `concierge_features` → personalised context, user-tier, environmental conditions.  
7. **Lineage & Audit:** full feature lineage logged to ADR-011.  
8. **Observability:** freshness, drift, and latency metrics to ADR-013.

---

## Consequences

**Positive**
- Reproducible, auditable AI features shared across models.  
- Real-time updates enable adaptive routing and recommendations.  
- Regional scalability with unified governance.  

**Trade-offs**
- Higher complexity in orchestration and metadata maintenance.  
- Requires coordinated feature versioning across teams.

---

## Implementation Details (High-level)

| Component | Responsibility | Notes |
|---|---|---|
| **Kafka / Event Bus** | Ingest telemetry, transport, weather, demand events | EU-resident cluster |
| **Databricks Pipelines** | Clean, transform, and aggregate features | Structured Streaming + Delta |
| **Feature Registry** | Store schema, version, freshness, owner | Delta table + metastore |
| **Offline Store (Delta Lake)** | Historical features for training | Supports reproducible snapshots |
| **Online Store (Redis/API)** | Real-time feature lookup for inference | Cached by region and model type |
| **Audit Log (ADR-011)** | Immutable feature lineage | Tamper-evident ledger |
| **Observability (ADR-013)** | Monitor feature latency, drift, and refresh | OTel-compliant metrics |
| **AI Interchange (ADR-014)** | Request real-time features by region/model | Shared interface across services |
| **Model Hosting (ADR-015)** | Consume offline features for training jobs | SageMaker EU region |
| **Concierge Service (ADR-009)** | Subscribe to feature updates and demand forecasts | Powers personalised advice |

**Sequence Example**

<img width="5037" height="1170" alt="16" src="https://github.com/user-attachments/assets/df19d35f-2fa2-48ec-ba94-77208cca694c" />

---

## Risks and Mitigations

| Risk                             | Likelihood | Impact | Mitigation                                 |
| -------------------------------- | ---------: | -----: | ------------------------------------------ |
| Provider data inconsistencies    |        Med |   High | Schema validation and registry enforcement |
| Weather API downtime             |        Low |    Med | Caching and fallback                       |
| Region misalignment              |        Med |    Med | Features keyed by `region_code`            |
| Feature drift or stale updates   |        Med |   High | Drift detection and freshness alerts       |
| Cross-model dependency conflicts |        Med |    Med | Governance board + CI feature validation   |

---

## Alternatives Considered

* Per-model pipelines – rejected for duplication and inconsistency.
* Batch-only feature store – rejected for latency and stale context.
* Unified real-time + batch store – chosen for scalability, reproducibility, and cross-model reuse.

## Links

* ADR-004 – Demand Prediction Model
* ADR-005 – Routing Solver (VRP-TW)
* ADR-007 – Data Platform Architecture
* ADR-009 – Concierge / Personalisation Service Architecture
* ADR-011 – Immutable Audit Log Architecture
* ADR-013 – Observability & Metrics Standardisation
* ADR-014 – AI Interchange Layer Architecture
* ADR-015 – Model Hosting & Inference Strategy

---
id: "0016"
title: AI Data Pipeline & Feature Store Design
status: proposed
date: 2025-10-22
---

## Context and Problem Statement

MobilityCorp’s AI ecosystem (forecasting, routing solver, anomaly detection, concierge service) depends on high-quality, reproducible, and auditable **features** derived from operational telemetry.  
Currently, model training pipelines are fragmented — each team builds its own ETL and stores derived features separately.  
This causes duplication, inconsistent feature definitions, and difficulty in reproducing model behaviour for compliance or retraining.

A unified **AI Data Pipeline and Feature Store** will ensure:
- **Consistent feature definitions** across models and teams.  
- **End-to-end lineage** from raw telemetry → feature → model input → inference output.  
- **EU data residency and governance compliance.**  
- **Reproducibility** for audits, debugging, and retraining.  
- **Tight integration** with the Data Platform (ADR-007), Audit Log (ADR-011), and Observability stack (ADR-013).  

---

## Questions

- Should features be materialised centrally (shared store) or built dynamically per model?  
- How do we ensure schema consistency across AI domains (routing, demand, anomaly)?  
- What is the right cadence for feature refresh (streaming vs batch)?  
- How do we version features and maintain lineage from telemetry to model?  
- How will this integrate with Databricks pipelines, Kafka streams, and the AI Interchange (ADR-014)?  

---

## Options

### Option A — Ad-hoc Feature Engineering (Status Quo)
Each team builds its own data prep pipeline and stores features in local datasets.

**Pros**
- High flexibility per use case.  
- No central coordination required.

**Cons**
- Duplicated logic and inconsistent semantics.  
- Difficult to reproduce models or audit feature lineage.  
- Slower model iteration and higher maintenance cost.

---

### Option B — Batch-Only Central Feature Repository
A nightly job computes and stores features in a shared warehouse.

**Pros**
- Central control and documentation.  
- Easy to audit and trace.  

**Cons**
- Not suitable for real-time inference.  
- Stale features for time-sensitive models (e.g., routing).  

---

### Option C — Unified Real-Time + Batch Feature Store (Recommended)
Adopt a **central Feature Store** built on the existing **Databricks Lakehouse** and **Kafka Event Bus (ADR-007)**, supporting both batch and streaming ingestion.  
Each feature is registered, versioned, and documented, with lineage tracking and audit integration.

**Pros**
- Real-time and offline parity (same feature definitions for training and inference).  
- Central schema and lineage; reproducible experiments.  
- Seamless integration with Databricks, AI Interchange (ADR-014), and SageMaker (ADR-015).  
- Automatic logging to the Immutable Audit Log (ADR-011).  

**Cons**
- Requires initial governance overhead.  
- Slightly more complexity in metadata management.  

---

## Recommendation

Adopt **Option C – Unified Real-Time + Batch Feature Store** as the core AI data backbone.

**Key decisions**
1. **Source Telemetry:** All raw events (routing, tasks, vehicle telemetry) ingested via **Kafka (ADR-007)**.  
2. **Transformation Layer:** Use Databricks structured streaming and Delta pipelines for cleaning, joining, and aggregating telemetry.  
3. **Feature Registry:** Central registry defines feature schemas, owners, freshness, and versions.  
4. **Materialisation:**  
   - Batch features → stored in Delta tables.  
   - Real-time features → served from Redis / online store API.  
5. **Lineage & Audit:** Each feature record linked to raw source and logged in **Audit Log (ADR-011)**.  
6. **Integration:**  
   - **Model Training (ADR-015)** reads from offline store.  
   - **AI Interchange (ADR-014)** pulls real-time features for inference.  
7. **Observability:** Metrics for data freshness, feature drift, and pipeline latency emitted under ADR-013 schema.

---

## Consequences

**Positive**
- Reproducible model training and explainability.  
- Streamlined cross-team collaboration via shared feature definitions.  
- Real-time and batch feature parity reduces deployment bugs.  
- Simplifies monitoring of drift and data quality.  

**Trade-offs**
- Increased governance and schema management workload.  
- Requires coordination between Data Engineering and ML Ops teams.  
- Initial migration of legacy pipelines to the unified schema.  

---

## Implementation Details (High-level)

| Component | Responsibility | Notes |
|---|---|---|
| **Kafka / Event Bus** | Ingest raw telemetry and operational data | Shared ingestion layer (ADR-007) |
| **Databricks Pipelines** | Transform telemetry into structured features | Streaming + batch modes |
| **Feature Registry** | Metadata, ownership, and versioning of features | Backed by Delta or Hive metastore |
| **Offline Store (Delta Lake)** | Persistent historical features for training | EU-resident; reproducible snapshots |
| **Online Store (Redis / API)** | Serve real-time features for inference | Cached from latest Kafka streams |
| **AI Interchange Layer (ADR-014)** | Consume features at inference time | Aligns schema between training/inference |
| **Audit Log (ADR-011)** | Immutable log of feature creation and updates | Hash-chained integrity |
| **Observability Stack (ADR-013)** | Monitor freshness, drift, and pipeline latency | OTel-compliant metrics |
| **Model Hosting (ADR-015)** | Consume registered features for inference | Via offline or real-time connectors |

**Sequence Example**


---

## Risks and Mitigations

| Risk                                          | Likelihood | Impact | Mitigation                                           |
| --------------------------------------------- | ---------: | -----: | ---------------------------------------------------- |
| Inconsistent feature definitions across teams |        Med |   High | Enforced schema and governance board                 |
| Data drift or stale features                  |        Med |   High | Freshness monitoring and drift detection alerts      |
| High latency in streaming pipelines           |        Med |    Med | Autoscaling clusters and micro-batch tuning          |
| Feature store downtime impacts inference      |        Low |   High | Cache fallback and replay from Kafka                 |
| Schema versioning conflicts                   |        Med |    Med | Versioned registry and backward-compatible contracts |
| Data residency violations                     |        Low |   High | EU region restriction and encrypted storage          |
| Cost creep due to redundant features          |        Med |    Med | Periodic feature usage audits                        |

---

## Alternatives Considered

* Ad-hoc pipelines per model – rejected for reproducibility issues.
* Batch-only central store – rejected for latency and stale data.
* Unified streaming + batch feature store – chosen for reproducibility, compliance, and cross-model reuse.

## Links

* ADR-007 – Data Platform Architecture
* ADR-011 – Immutable Audit Log Architecture
* ADR-013 – Observability & Metrics Standardisation
* ADR-014 – AI Interchange Layer Architecture
* ADR-015 – Model Hosting & Inference Strategy
* ADR-005 – Routing Solver (VRP-TW)
* ADR-006 – Alerts & Theft Detection Logic
