---
id: "0004"
title: Demand Prediction Model Design
status: proposed
date: 2025-10-18
---

## Context and Problem Statement

MobilityCorp needs to **predict demand at hubs over time** to:
- Pre-position vehicles and batteries (reduce empty/full hubs).
- Generate **time-windowed** field-ops routes (VRP-TW).
- Power **destination-based** user flow and **concierge** recommendations.
- Minimise **ops costs** and improve **utilisation**.

Constraints and realities:
- **IoT telemetry** is provided by an **external platform** (variable latency/frequency).
- **EU-only, GDPR** baseline: personalisation is **opt-in**; predictions must be explainable at an aggregate level.
- We must control **AI cost risk** and degrade gracefully to heuristics when models or providers fail.
- Forecasts must be available on an **hourly rolling horizon** (e.g., next 24–72h) with **city/hub granularity**.

### Questions
- What modelling approach balances **accuracy, cost, and latency**?
- How do we incorporate **external signals** (weather, holidays, transit strikes, events)?
- How do we avoid **vendor lock-in** and enable **fallback heuristics**?
- Where do **explanations** and **governance** (drift, retraining, budgets) live?

---

## Options

### Option A — Heuristic / rules-only baseline
Moving averages + seasonality factors (hour-of-day, day-of-week), recent trend multipliers, weather sensitivity coefficients.

**Pros**
- Very cheap, fast, transparent, and robust.
- No ML platform dependency.

**Cons**
- Lower accuracy in volatile patterns (events/strikes).
- Manual tuning; limited adaptation.

---

### Option B — Classical time-series / tabular ML
Per-hub models or pooled model using **XGBoost/LightGBM** (tabular) and/or **Prophet**/**SARIMAX** for seasonality; features include:
- Historical demand (lagged), calendar/seasonality, **weather**, **public holidays**, **planned events/strikes**, hub capacity/footfall proxies.

**Pros**
- Strong accuracy/cost trade-off; fast training/inference on CPU.
- Feature-driven and explainable (feature importance, SHAP).
- Easy to run on our **own infra** (EU-resident), avoiding provider risk.

**Cons**
- Needs feature engineering and data pipeline discipline.
- Per-hub modelling can be operationally heavier (mitigated via pooled models).

---

### Option C — Deep learning (LSTM/Temporal Fusion Transformer)
End-to-end neural forecasting (per hub or multi-horizon).

**Pros**
- Can capture complex interactions and long-range patterns.

**Cons**
- Higher compute cost and ops complexity; explainability weaker.
- Overkill initially; risk of AI cost overruns.

---

### Option D — Third-party forecasting API
Outsource forecasting to a SaaS provider.

**Pros**
- Minimal build effort.

**Cons**
- **Data residency**/GDPR risk; vendor dependency and **financial risk**.
- Limited control over features and explanations.

---

## Decision

Adopt **Option B: Classical time-series/tabular ML** as the **primary model family**, with:
- **Option A** as the **deterministic fallback** (cost/availability guardrail).
- Reserve **Option C** experiments for specific cities if measurable lift and cost within budget (see ADR-019).

Implementation stance:
- Serve forecasts from an internal **Prediction Service** behind the **AI Interchange Layer** (ADR-015).
- Train and host models on **EU-resident infra** (ADR-017) using the **Data Platform & Feature Store** (ADR-017).
- Provide **explanations** (feature importance/SHAP summaries) for auditing (ADR-018).

---

## Consequences

**Positive**
- Strong accuracy vs cost; CPU-friendly inference (scales to many hubs).
- Transparent and **explainable** (required for GDPR-aligned decisions).
- Minimal vendor lock-in; can be fully self-hosted in EU regions.
- Clean fallback path to **heuristics** during outages or budget caps.

**Trade-offs**
- Requires disciplined **feature engineering** and **data quality** monitoring.
- Per-hub models may need lifecycle automation (training orchestration, model registry).
- Less cutting-edge than deep models; may miss rare complex patterns (acceptable initially).

---

## Implementation Details (High-level)

### Scope and Interfaces
- **Inputs**: hourly aggregated demand per hub (rides started/ended), hub metadata (capacity/targets), weather (temp, precipitation, wind), calendar (DoW, holidays), events/strikes feed, recent telemetry freshness.
- **Outputs**: `forecast[horizon_hours]` per hub with confidence intervals (P50, P90).
- **Consumers**: Routing/VRP (ADR-005), Field Ops UI, Concierge, Back Office dashboards.

### Data & Features
- Lagged targets (t-1…t-168), rolling means, trend.
- Categorical: hub_id, city, neighbourhood.
- Exogenous: weather (hourly forecast), holiday flags, event density/impact score.
- Data retention aligned to GDPR: aggregate demand is non-PII; ensure opt-in for **user-level** concierge features.

### Training & Serving
- **Cadence**: daily retrain + hourly incremental refresh (online features).
- **Serving**: CPU endpoints with p95 < 100ms per hub batch; batch scoring for horizons.
- **Registry**: model/version registry with metadata (data window, features, metrics).

### Monitoring & Governance
- **Accuracy**: WAPE/SMAPE per hub; alert on drift
- **Data drift**: population stability index on key features.
- **Cost**: track CPU hours / job costs; enforce monthly budget caps (ADR-018).
- **Explainability**: store per-forecast SHAP summaries; expose `/explain` endpoint.
- **Fallback**: if accuracy < threshold or cost > budget, automatically switch to **heuristic baseline** and notify Ops.

sequenceDiagram
    title Demand Prediction → Routing → Field Ops Workflow

    participant IoT as IoT Platform
    participant DP as Data Platform / Feature Store
    participant PM as Prediction Service (Forecast Model)
    participant AI as AI Interchange Layer
    participant RT as Routing Service (VRP-TW Solver)
    participant FO as Field Ops App
    participant BO as Back Office

    %% Telemetry and data ingestion
    IoT-->>DP: Stream telemetry (location, battery, usage)
    DP-->>PM: Aggregate demand, join with weather + events
    PM-->>AI: Publish forecast {hub_id, horizon, demand_curve}

    %% Prediction hand-off
    AI-->>RT: Provide demand forecast API (per city)
    RT-->>DP: Request live hub inventory, demand forecast

    %% Routing & assignment
    RT->RT: Solve Vehicle Routing Problem (multi-vehicle, time windows)
    RT-->>AI: Return route plan metadata
    AI-->>FO: Push optimised route assignment (via notification topic)
    FO-->>RT: ACK route acceptance / progress updates

    %% Monitoring & feedback loop
    FO-->>DP: Report task completion, timing, battery swaps
    DP-->>PM: Feed outcomes for retraining (actual vs predicted)
    PM-->>BO: Metrics (forecast accuracy, utilisation, SLA)

    %% Back office supervision
    BO-->>AI: Adjust constraints (e.g., fleet size, shift window)
    AI-->>RT: Re-trigger route optimisation with new parameters


---

## Risks and Mitigations

- **AI model degradation / drift** → Continuous monitoring; auto-retrain; fallback to heuristics.
- **Financial risk (compute spikes)** → Budget ceilings; batch scoring; pooled models.
- **Vendor risk (hosting/model tooling)** → Self-hosted preferred; portable artifacts (ONNX/PMML); exportable weights.
- **Bias / unfair allocation** (over-serving affluent areas) → Add fairness constraints; periodic human review.
- **Data quality** (missing exogenous feeds) → Graceful degradation; impute; drop features with alerts.

---

## Alternatives Considered

- **Full deep-learning first**: rejected initially due to cost/ops complexity and explainability needs.
- **Third-party SaaS**: rejected for data residency, lock-in, and cost opacity.

---

## Links

- Relates to **ADR-0005** (Routing Solver), **ADR-0014** (AI Interchange), **ADR-0015** (Model Hosting), **ADR-0016** (Feature Store), **ADR-0017** (AI Governance & Explainability), **ADR-0018** (AI Cost Guardrails), **ADR-0007** (Data Platform).

---