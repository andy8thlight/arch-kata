---
id: "0013"
title: Observability & Metrics Standardisation
status: proposed
date: 2025-10-21
---

## Context and Problem Statement

As MobilityCorp’s platform expands (routing, mobile sync, concierge AI, billing), operational complexity increases across multiple services and data domains.  
At present, **metrics, logs, and traces are fragmented**—each service emits telemetry in its own structure, making it difficult to measure reliability, latency, and business SLOs consistently.

To ensure **reliability, accountability, and continuous improvement**, the organisation requires a **standardised observability framework** that:

- Defines a **unified schema** for logs, traces, and metrics across all services.  
- Enables cross-service correlation (e.g., trace a booking through solver, hub reservation, and audit).  
- Supports **SLO/SLA monitoring**, incident triage, and root-cause analysis.  
- Feeds aggregated data into **Databricks (ADR-007)** for analytics and long-term trend analysis.  

---

## Questions

- Should we adopt an existing standard (OpenTelemetry, Prometheus, Elastic APM) or build a lightweight internal schema?  
- How do we ensure consistent correlation IDs across microservices?  
- What metrics are mandatory (latency, solver time, booking errors, sync retries)?  
- How do we expose SLOs to dashboards and alerting tools (Grafana, Datadog, or internal UI)?  
- How can we maintain EU data residency for production logs while allowing anonymised analytics?  

---

## Options

### Option A — Service-Specific Metrics
Each service defines and emits its own telemetry independently.

**Pros**
- No coordination required.  
- Fast for individual teams to implement.

**Cons**
- No unified visibility or cross-trace correlation.  
- Hard to measure global reliability or enforce SLOs.  
- Duplicate effort and inconsistent naming.

---

### Option B — Centralised Logging Only
Forward all logs to a single collector (e.g., ELK or Datadog) for aggregation and search.

**Pros**
- Central visibility and long-term retention.  
- Quick to implement.  

**Cons**
- Metrics and traces still inconsistent.  
- No structured schema or SLO context.  
- High ingestion cost without filtering.

---

### Option C — Unified Observability Standard (Recommended)
Adopt **OpenTelemetry (OTel)** for metrics, logs, and traces with a **MobilityCorp-defined schema extension**.  
All services instrumented with standard OTel SDKs publish to a central collector, which exports to multiple sinks (Databricks, Prometheus, Grafana, SIEM).

**Pros**
- End-to-end traceability across services.  
- Standardised metric names, units, and labels.  
- Easy integration with open-source and commercial monitoring tools.  
- Enables automated SLO dashboards.  

**Cons**
- Requires instrumentation effort across all teams.  
- Additional infrastructure for collectors and exporters.

---

## Recommendation

Adopt **Option C – Unified Observability Standard using OpenTelemetry**.

Key design elements:
1. **Schema Alignment:** Define MobilityCorp telemetry schema (`service`, `operation`, `correlation_id`, `latency_ms`, `error_code`, `user_tier`, etc.).  
2. **Cross-Service Correlation:** All logs/traces include a shared `trace_id` and `span_id`.  
3. **Metrics Aggregation:** Use Prometheus format for counters, gauges, and histograms.  
4. **Export Pipeline:**  
   - Real-time monitoring → Prometheus / Grafana.  
   - Long-term storage → Databricks (ADR-007).  
   - Security and compliance events → Audit Log (ADR-011).  
5. **SLO Enforcement:** Define service-level objectives for latency, uptime, solver response, booking success, and error budgets.  
6. **Alerting Integration:** Threshold-based alerts routed via OpsCenter / Slack / PagerDuty.

---

## Consequences

**Positive**
- Unified telemetry and consistent reliability metrics.  
- Enables system-wide SLO monitoring and trend analysis.  
- Reduces MTTR (mean time to resolution) during incidents.  
- Provides quantitative evidence for quality and SLA reporting.  

**Trade-offs**
- Initial instrumentation workload across existing services.  
- Requires collector and dashboard maintenance.  
- Some metrics duplication between observability and analytics pipelines.

---

## Implementation Details (High-level)

| Component | Responsibility | Notes |
|---|---|---|
| **Service SDKs (OTel)** | Emit metrics, logs, and traces using standard schema | Common library per language |
| **OTel Collector / Agent** | Receive telemetry from services, apply filters and transforms | Deployed per cluster or region |
| **Prometheus / Grafana** | Real-time metrics and dashboard visualisation | Supports SLO thresholds |
| **Databricks Lakehouse** | Long-term storage and analytics | EU-resident; aligns with ADR-007 |
| **SIEM / Alerting System** | Security and anomaly detection | Integrates with InfoSec tooling |
| **Audit Log (ADR-011)** | Immutable record of config changes and incident reviews | Tamper-evident verification |
| **OpsCenter Dashboard** | Aggregated SLOs, latency, and error trends | Used by Reliability & Ops teams |

**Sequence Example**

<img width="3406" height="978" alt="13" src="https://github.com/user-attachments/assets/3a90297c-944d-4327-a064-8f50dc28ffa2" />

---

## Risks and Mitigations

| Risk                                   | Likelihood | Impact | Mitigation                                         |
| -------------------------------------- | ---------: | -----: | -------------------------------------------------- |
| Inconsistent trace IDs across services |        Med |   High | Use standard middleware for correlation IDs        |
| Excessive telemetry volume             |        Med |    Med | Sampling and rate-limiting at collector            |
| Misconfigured metrics or labels        |        Low |    Med | Schema validation CI checks                        |
| Data residency breaches                |        Low |   High | EU-only collectors and storage                     |
| Alert fatigue from noisy thresholds    |        Med |    Med | Define SLO-based alerting and suppression policies |
| Tooling fragmentation                  |        Med |    Med | Centralised observability governance board         |

---

## Alternatives Considered

* Maintain ad-hoc service metrics – rejected for lack of cross-service visibility.
* Centralised logs without traces – rejected for weak correlation.
* Unified OTel-based observability – chosen for standardisation, scalability, and ecosystem compatibility.

## Links

* ADR-005 – Routing Solver (VRP-TW)
* ADR-007 – Data Platform Architecture
* ADR-008 – Mobile Offline Sync Strategy
* ADR-009 – Personalisation / Concierge Service Architecture
* ADR-011 – Immutable Audit Log Architecture
* ADR-012 – Premium Subscription Model Integration
* ADR-016 – AI Interchange Layer
