---
id: "0017"
title: AI Governance & Explainability Framework
status: proposed
date: 2025-10-22
---

## Context and Problem Statement

MobilityCorp’s AI systems — routing optimisation, anomaly detection, concierge recommendations, and predictive forecasting — increasingly influence operational and customer outcomes.  
Under **GDPR Article 22**, individuals have the right **not to be subject to solely automated decisions** that significantly affect them without transparency and the ability to contest outcomes.

Current AI services provide limited auditability beyond basic logs.  
A consistent **AI Governance & Explainability Framework** is required to:

- Capture **who/what/why** behind every model decision.  
- Provide **human-readable explanations** for affected users and regulators.  
- Ensure **traceability** between inputs, features, models, and outputs.  
- Integrate seamlessly with **Audit Log (ADR-011)** and **AI Interchange (ADR-014)**.  
- Demonstrate compliance with internal ethics policies and external regulations.

---

## Questions

- What data must be stored to reconstruct or explain any model decision?  
- Should explanations be generated by the model itself or a parallel interpretable layer?  
- How do we provide user-level access to decision explanations without leaking PII?  
- How do we align cost, storage, and retention with GDPR principles (data minimisation, purpose limitation)?  
- How are governance policies versioned, enforced, and monitored across all AI endpoints?

---

## Options

### Option A — Model-Specific Explainability Only
Each team implements its own explanation logic per model (e.g., SHAP plots, saliency maps).

**Pros**
- High flexibility per use case.  
- No central dependency.

**Cons**
- Inconsistent outputs and terminology.  
- Hard to audit or aggregate across services.  
- Poor governance visibility.

---

### Option B — Post-hoc Logging Without Structure
Store model inputs/outputs in raw logs for later inspection.

**Pros**
- Minimal setup.  
- Compatible with existing observability stack.

**Cons**
- No standard schema or linkage to features/models.  
- Difficult to reconstruct decisions reliably.  
- Fails GDPR explainability expectations.

---

### Option C — Central AI Governance & Explainability Layer (Recommended)
Implement a **central governance layer** integrated into the **AI Interchange (ADR-014)**.  
This layer captures, verifies, and exposes explainability metadata for every AI inference, while enforcing consent and transparency policies.

**Pros**
- Unified governance, schema, and access controls.  
- Enables individual right-to-explanation fulfilment.  
- Strong linkage to Audit Log (ADR-011) and Data Platform (ADR-007).  
- Scalable and consistent across models.  

**Cons**
- Requires integration effort and storage governance.  
- Slight inference latency increase from metadata capture.

---

## Recommendation

Adopt **Option C – Central AI Governance & Explainability Layer**.

**Key design decisions**
1. **Explainability Envelope:** every AI response includes a governance header containing model ID, version, timestamp, and hash of input/feature set.  
2. **Explainability Service:** generates summaries using model-agnostic interpreters (e.g., SHAP, LIME) or predefined rule templates.  
3. **Audit Integration:** all inference events and explanation artifacts logged to **Immutable Audit Log (ADR-011)**.  
4. **User Access API:** exposes GDPR Art. 22 explanation endpoint via **AI Interchange (ADR-014)**.  
5. **Governance Registry:** stores approved models, risk classifications, and explanation policies.  
6. **Metrics & Monitoring:** feed decision counts, latency, and explanation quality into **Observability (ADR-013)**.  
7. **Compliance Interface:** dashboards and exports for internal Ethics Board and external auditors.

---

## Consequences

**Positive**
- Demonstrable compliance with GDPR Art. 22 and internal AI policy.  
- Full traceability of model lineage, inputs, and outputs.  
- Human-interpretable explanations available on demand.  
- Foundation for ethical reviews and model-risk scoring.

**Trade-offs**
- Additional infrastructure and storage overhead.  
- Requires policy lifecycle management (versioning, expiry).  
- Latency impact for real-time explainability generation.

---

## Implementation Details (High-level)

| Component | Responsibility | Notes |
|---|---|---|
| **AI Interchange (ADR-014)** | Entry point for all model requests; attaches governance metadata | Validates consent and tier |
| **Explainability Service** | Generates human-readable model explanations | Uses SHAP/LIME templates or rule logic |
| **Governance Registry** | Stores model metadata, risk ratings, and policy versions | Managed by Compliance & AI Ops |
| **Audit Log (ADR-011)** | Immutable record of decisions and explanations | Hash-chained ledger |
| **Data Platform (ADR-007)** | Stores anonymised training/evaluation data | Enables retraining & reproducibility |
| **Observability (ADR-013)** | Collects latency and compliance metrics | Unified telemetry schema |
| **User Access API** | Serves GDPR explanation requests | Authenticated & redacted responses |

**Sequence Example**

<img width="3879" height="1170" alt="17" src="https://github.com/user-attachments/assets/a534ee47-2180-45ae-81dd-122deca5efe1" />

---

## Risks and Mitigations

| Risk                                       | Likelihood | Impact | Mitigation                                   |
| ------------------------------------------ | ---------: | -----: | -------------------------------------------- |
| High storage overhead for explanations     |        Med |    Med | Sampling, TTL expiry, compression            |
| Inconsistent explanation quality           |        Med |    Med | Central templates and validation tests       |
| GDPR data access abuse                     |        Low |   High | Authenticated, audited user API              |
| Model updates invalidate old explanations  |        Med |   High | Versioned policies and re-generation jobs    |
| Latency from inline explanation generation |        Med |    Med | Async explanation pipeline with caching      |
| Governance drift between teams             |        Med |   High | Compliance reviews and model registry audits |

---

## Alternatives Considered

* Per-model explainers – rejected for inconsistency.
* Post-hoc logs without schema – rejected for weak traceability.
* Central governance + explainability layer – chosen for compliance, scalability, and audit integration.

## Links

* ADR-007 – Data Platform Architecture
* ADR-011 – Immutable Audit Log Architecture
* ADR-013 – Observability & Metrics Standardisation
* ADR-014 – AI Interchange Layer Architecture
* ADR-016 – AI Data Pipeline & Feature Store Design
* ADR-012 – Premium Subscription Model Integration
