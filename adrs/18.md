---
id: "0018"
title: AI Cost & Budget Guardrails
status: proposed
date: 2025-10-22
---

## Context and Problem Statement

As MobilityCorp’s AI footprint expands (forecasting, routing, concierge, anomaly detection), inference and training workloads increasingly impact operating budgets.  
Currently, cost visibility is fragmented — each service reports token usage, compute time, or model-endpoint cost differently.  
To sustain transparency and predictable spend, we require **AI Cost & Budget Guardrails** that:

- Provide **real-time cost monitoring** across inference and training.  
- Enforce **per-service and per-team budgets**.  
- Integrate with **Observability (ADR-013)** and **AI Interchange (ADR-014)** for unified telemetry.  
- Enable **forecasting and alerts** before overruns occur.  
- Support executive and finance dashboards for tracking spend vs. value.

---

## Questions

- Should cost limits be enforced centrally at the AI Interchange layer or individually per service?  
- How granular should budgets be — team, model, environment, or feature level?  
- How frequently should cost data sync with Databricks for analytics?  
- How do we handle provider pricing changes or token-to-currency conversions?  
- How do we reconcile cost governance with model-performance optimisation (e.g., larger LLMs)?  

---

## Options

### Option A — Manual Cost Tracking
Each team monitors its own inference and training spend manually through billing dashboards.

**Pros**
- No new infrastructure.  
- Immediate visibility for small teams.

**Cons**
- No enforcement or central alerting.  
- Inconsistent cost accounting across environments.  
- Poor forecasting accuracy.

---

### Option B — Offline Cost Reporting Jobs
Nightly ETL aggregates usage metrics (tokens, compute hours) into Databricks for review.

**Pros**
- Simple batch process; minimal runtime overhead.  
- Enables centralised financial reporting.

**Cons**
- Lacks real-time visibility or enforcement.  
- Alerts occur after overspend.  

---

### Option C — Central AI Cost Governance Layer (Recommended)
Implement a **real-time cost-governance component** within the **AI Interchange (ADR-014)**.  
All inference and training jobs emit usage telemetry that is normalised, budget-checked, and exported to Databricks and the finance dashboard.

**Pros**
- Unified real-time visibility and control.  
- Per-model, per-team, or per-tenant budgets.  
- Automatic throttling or graceful degradation when thresholds are reached.  
- Integrates natively with Observability (ADR-013) and Audit Log (ADR-011).  

**Cons**
- Requires additional monitoring logic and coordination with finance systems.  
- Potential minor latency overhead for telemetry emission.

---

## Recommendation

Adopt **Option C – Central AI Cost Governance Layer** integrated with the **AI Interchange (ADR-014)**.

**Key decisions**
1. **Unified Cost Schema:** standard fields – `model_id`, `provider`, `tokens`, `latency_ms`, `cost_usd`, `team_id`, `budget_id`.  
2. **Budget Enforcement:** configurable per team, environment, or model; enforced at request time in the Interchange.  
3. **Real-Time Telemetry:** every inference call and training job emits cost data to Kafka → Databricks → dashboards.  
4. **Forecasting & Alerts:** Databricks notebooks compute cost trends and send alerts to Slack/Email when usage exceeds thresholds.  
5. **Audit Integration:** all cost events hash-logged to the **Immutable Audit Log (ADR-011)**.  
6. **Observability:** expose cost metrics (per-minute usage, spend rate) via OTel for dashboards (ADR-013).  
7. **Governance Dashboard:** finance and engineering share a single view of AI spend, usage, and remaining budget.

---

## Consequences

**Positive**
- Predictable, controlled AI operating costs.  
- Single source of truth for cost attribution and optimisation.  
- Supports financial accountability across teams.  
- Enables proactive budget management and forecasting.

**Trade-offs**
- Additional engineering and telemetry overhead.  
- Requires cross-team agreement on budget hierarchies.  
- Potential throttling impact if budgets are mis-configured.

---

## Implementation Details (High-level)

| Component | Responsibility | Notes |
|---|---|---|
| **AI Interchange (ADR-014)** | Emit usage and cost metrics per inference | Central policy enforcement |
| **Model Hosting (ADR-015)** | Report training and endpoint compute usage | AWS SageMaker EU region |
| **Kafka / Event Bus** | Transport cost telemetry events | Shared ingestion layer (ADR-007) |
| **Databricks Lakehouse** | Aggregate and analyse cost data | Basis for dashboards and alerts |
| **Budget Registry** | Store budget definitions per team/model | API-managed; integrated with Finance |
| **Audit Log (ADR-011)** | Immutable ledger of cost events and policy actions | Tamper-evident chain |
| **Observability (ADR-013)** | Display real-time spend metrics | Unified dashboards |
| **Notification Service** | Send alerts on threshold breaches | Slack/Email/PagerDuty integration |

**Sequence Example**

<img width="3805" height="1074" alt="18" src="https://github.com/user-attachments/assets/33961f10-b747-42f9-9122-9760c20d4a1e" />

---

## Risks and Mitigations

| Risk                                        | Likelihood | Impact | Mitigation                                  |
| ------------------------------------------- | ---------: | -----: | ------------------------------------------- |
| Budget enforcement false-positives          |        Med |    Med | Grace period + manual override              |
| High telemetry volume                       |        Med |    Med | Sampling, aggregation, rate limiting        |
| Cost under-reporting due to provider errors |        Low |   High | Reconcile nightly with billing APIs         |
| Inconsistent budget ownership               |        Med |    Med | Clear registry ownership by Finance         |
| Latency impact from real-time checks        |        Low |    Med | Async queueing and non-blocking evaluation  |
| Alert fatigue                               |        Med |    Med | Tiered thresholds and notification grouping |

---

## Alternatives Considered

* Manual cost monitoring – rejected for lack of enforcement.
* Offline nightly ETL – rejected for latency and delayed visibility.
* Centralised real-time governance – chosen for unified control and proactive cost management.

## Links

* ADR-007 – Data Platform Architecture
* ADR-011 – Immutable Audit Log Architecture
* ADR-013 – Observability & Metrics Standardisation
* ADR-014 – AI Interchange Layer Architecture
* ADR-015 – Model Hosting & Inference Strategy
* ADR-016 – AI Data Pipeline & Feature Store Design
* ADR-017 – AI Governance & Explainability Framework
