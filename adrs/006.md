---
id: "0006"
title: Alerts & Theft Detection Logic
status: proposed
date: 2025-10-21
---


## Context and Problem Statement

MobilityCorp, as a multi-region ride sharing and fleet operations platform, must detect and respond to **vehicle theft, unauthorised use, and operational anomalies** in near real time. This capability is essential for fleet protection, insurance compliance, and customer trust.

The system must:
- Ingest live telemetry and status updates from vehicles and field devices.
- Detect suspicious behaviour (e.g., ignition without authorisation, location mismatch, route deviation).
- Trigger alerts and automated containment actions (lockdown, disable ignition, notify authorities).
- Maintain **auditability** and **traceability** for all alerts and responses.
- Integrate with the event-driven data platform ([ADR-007](./007.md)) for unified ingestion, storage, and analytics.

The challenge is to balance **detection sensitivity, false positives, and latency**, while maintaining GDPR compliance and operational resilience. This ADR builds on the event-driven, reliable integration patterns established in [ADR-001](./001.md), [ADR-002](./002.md), and [ADR-007](./007.md), ensuring that alerting and detection logic is robust, auditable, and seamlessly integrated with upstream and downstream operational systems.

---


### Key Questions

- Should anomaly detection be fully rule-based, fully AI-driven, or hybrid?  
- How close to real time should detection operate (edge vs central)?  
- How do we manage false positives without desensitising operators?  
- Should historical alert patterns inform future detection logic (vector retrieval)?  
- What mechanisms ensure reliable alert delivery to downstream systems?

---


## Options

### Option A — Rule-Based Engine Only
Define static rules (e.g., “movement outside assigned zone”, “ignition without key”) evaluated on incoming telemetry.

**Pros**
- Simple, deterministic, auditable.  
- Easy to reason about and certify.

**Cons**
- Brittle; requires frequent tuning.  
- Cannot generalise to unseen anomaly types.  

---

### Option B — ML-Driven Anomaly Detection
Train ML models to detect deviations in multi-dimensional telemetry streams.

**Pros**
- Adapts to new and subtle anomalies.  
- Scalable across fleet and geographies.

**Cons**
- Requires labelled historical data.  
- Less transparent; harder to explain results.  
- Higher compute cost and maintenance overhead.  

---


### Option C — Hybrid Approach (Recommended)
Combine deterministic rules with ML anomaly detection, orchestrated via the shared event-driven data platform ([ADR-007](./007.md)). Augment with **optional Vector DB retrieval** to provide context (“find similar historical events”) for operators and automated triage.

**Pros**
- Fast deterministic response with adaptive intelligence.
- Explainability through contextual examples.
- Naturally fits into Kafka → Databricks → AI pipeline.

**Cons**
- Slightly higher engineering complexity.
- Requires embedding and retrieval pipeline if Vector DB is activated.

---


## Decision

We will adopt **Option C – Hybrid Rule + ML Detection** for MobilityCorp’s alerts and theft detection logic, leveraging shared infrastructure components and robust governance. This approach aligns with the platform’s commitment to operational excellence, compliance, and architectural consistency across domains.

**Rationale:**
- Hybrid detection (rules + ML) provides both fast, deterministic response and adaptive intelligence, supporting real-time fleet protection and operational resilience.
- Integration with the event-driven data platform ([ADR-007](./007.md)), feature store, and audit log ([ADR-013](./013.md)) ensures robust, auditable, and explainable detection workflows.
- Optional vector retrieval supports operator explainability and future extensibility.
- While this approach introduces some engineering complexity, the benefits in accuracy, transparency, and compliance outweigh the trade-offs for a multi-region mobility platform.

Options A and B were rejected because they do not provide sufficient adaptability, transparency, or operational resilience. Option A (rules only) is too brittle, while Option B (ML only) is less explainable and harder to govern.

**Key decisions:**
- **Kafka Event Bus** – Vehicle telemetry and operational events ([ADR-007](./007.md)).
- **Databricks Lakehouse** – Aggregated features, training datasets, historical anomalies.
- **Feature Store** – Normalisation and model input features.
- **Anomaly Detection Service** – Combines rule evaluation with ML inference.
- **Vector Database (optional)** – Semantic retrieval of similar past alerts for context and operator insight.
- **Alert Orchestrator** – Routes verified alerts to the Operations Portal, Notification Service, and Audit Log ([ADR-013](./013.md)).

---


## Consequences

**Positive Consequences:**
- Real-time theft and anomaly detection across fleet, improving asset protection and customer trust.
- Explainable, auditable decisions, supporting compliance and post-incident review.
- Extensible for future AI models and retrieval-based explainability.

**Negative Consequences / Trade-offs:**
- Slightly more operational complexity than rule-only system.
- Requires additional monitoring of model performance and drift.
- Optional vector layer introduces governance overhead if activated.

**Stakeholders:**
- Platform Engineering: responsible for detection service development, integration, and maintenance.
- Data Engineering: responsible for telemetry pipelines, feature store, and data quality.
- Operations: define business requirements, validate alert outcomes, and monitor response SLAs.
- Compliance and Legal: oversee GDPR, audit, and regulatory requirements.


**Reliability and Integration:**
- All alert events, detection outcomes, and escalations must use reliable, idempotent delivery patterns as established in [ADR-001](./001.md) (e.g., Outbox + Worker Queue for Kafka topics).
- Downstream consumers must be able to handle retries, late or missing events, and ensure at-least-once processing semantics.
- If alert delivery to downstream systems fails, the Alert Orchestrator will retry delivery with exponential backoff and raise an incident if delivery cannot be confirmed within SLA. Escalation to on-call engineering and operations is required for persistent failures.


**Audit and Compliance:**
- All detection decisions, triggers, and actions must be logged to the Immutable Audit Log ([ADR-013](./013.md)) and be accessible for compliance review.
- Regular audits and SLA reviews must be scheduled and owned by Platform Engineering and Compliance teams. Audit frequency: quarterly, or after any major incident. Compliance team is responsible for regulatory monitoring and escalation.
- GDPR compliance is ensured by pseudonymising and aggregating telemetry before storage, maintaining a data minimization policy, and supporting subject access requests. Compliance team will conduct annual GDPR reviews and respond to regulatory inquiries.


**Risk Ownership and Feedback Loops:**
- Platform Engineering is responsible for monitoring detection performance, false positives/negatives, and integration health. A feedback loop is established: all operator feedback and post-incident reviews are logged and reviewed monthly to tune thresholds, retrain models, and update rules.
- Compliance team is responsible for GDPR, audit, and regulatory risks. Escalation path: incident review → compliance lead → executive sponsor.


**Implementation and Rollout Plan:**
1. Develop and document canonical event schemas for all alert-related topics.
2. Build and validate rule and ML-based detection pipelines.
3. Integrate with the event-driven data platform ([ADR-007](./007.md)) and ensure reliable event delivery.
4. Register all detection models and log all alert events to the Audit Log ([ADR-013](./013.md)).
5. Establish monitoring dashboards and alerting for detection health, latency, and event delivery failures.
6. Communicate rollout plan, milestones, and responsibilities to all stakeholders via regular project updates, incident postmortems, and review meetings. Stakeholder communication will occur at project kickoff, before go-live, after major incidents, and at each quarterly review.
7. Define and document the incident response process for critical alerts, including escalation to operations, compliance, and executive sponsors.


**Follow-ups / Open Issues:**
- Finalize event schemas and data contracts with all consuming and producing services.
- Define SLAs for detection latency, reliability, and auditability.
- Review and update compliance documentation as regulations evolve.
- Confirm stakeholder sign-off at each rollout milestone.
- Track open integration issues with telemetry providers and update risk register as needed.
- Define and document the process for activating the vector database: it should be enabled when operator feedback or incident reviews indicate a need for improved explainability or when regulatory requirements demand enhanced context for alerts. The decision to activate is made jointly by Platform Engineering, Operations, and Compliance.

---

## Implementation Details (High-level)

| Component | Responsibility | Notes |
|---|---|---|
| **Kafka / Event Bus** | Ingest real-time telemetry and status data | Shared ingestion layer ([ADR-007](./007.md)) |
| **Databricks Lakehouse** | Aggregate and store historical events | Used for ML training and retrospective analysis |
| **Feature Store** | Curate and normalise features for ML inference | Shared with other AI services |
| **Anomaly Detection Service** | Run rules and ML models; generate alerts | Rule engine + model ensemble |
| **Vector Database (optional)** | Retrieve semantically similar historical alerts | Provides operator context and explainability |
| **Alert Orchestrator** | Deduplicate, prioritise, and route alerts | Publishes verified alerts to downstream systems |
| **Audit Log ([ADR-013](./013.md))** | Record detection decisions, triggers, and actions | Immutable; supports post-incident review |
| **Notification Service** | Notify operators, authorities, or automated systems | Configurable escalation paths |

---

## Sequence Diagram 


<img width="5210" height="1600" alt="006" src="https://github.com/user-attachments/assets/c0bd78d8-2b32-412d-821d-2e4794c5b937" />

---

## Risks and Mitigations

| Risk                                   | Likelihood | Impact | Mitigation                                                     |
| -------------------------------------- | ---------: | -----: | -------------------------------------------------------------- |
| False positives causing alert fatigue  |        Med |   High | Threshold tuning, contextual filtering, operator feedback loop |
| Missed anomalies (false negatives)     |        Med |   High | Model retraining, ensemble rules                               |
| Latency under heavy telemetry load     |        Med |    Med | Stream partitioning, horizontal scaling                        |
| GDPR exposure from raw telemetry       |        Low |   High | Pseudonymise and aggregate before storage                      |
| Misclassification of legitimate events |        Low |    Med | Manual review queue and audit trail                            |
| Cost of optional vector retrieval      |        Low |    Low | Activate only for explainability use cases                     |


---


## Alternatives Considered

- **Pure rule-based detection** – simple but too brittle for real-world variability.
- **ML-only detection** – powerful but opaque and harder to govern.
- **Hybrid approach with contextual retrieval** – chosen for balanced performance, transparency, and extensibility.


## Related ADRs

- [ADR-001 – External Dependency SLA & Retry Handling](./001.md)
- [ADR-002 – Pricing & Billing Microservice Architecture](./002.md)
- [ADR-005 – Routing Solver (VRP-TW)](./005.md)
- [ADR-007 – Data Platform Architecture](./007.md)
- [ADR-008 – Mobile Offline Sync Strategy](./008.md)
- [ADR-009 – Personalisation / Concierge Service Architecture](./009.md)
- [ADR-013 – Immutable Audit Log Architecture](./013.md)
- [ADR-016 – AI Interchange Layer](./016.md)
