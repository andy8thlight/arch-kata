---
id: "0001"
title: External Dependency SLA & Retry Handling
status: proposed
date: 2025-10-17
---

## Context and Problem Statement

MobilityCorp depends on several **external systems** outside our operational control:

- **IoT Integration API** – for telemetry, lock/unlock commands, and vehicle status.  
- **Payment Provider** – for pre-authorisations, captures, and refunds.  
- **Identity Verification (IDV / KYC)** – for licence and age validation.  

Each dependency introduces **latency, rate-limits, and failure modes** that can affect the reliability of our critical user flows (starting rides, ending rides, charging customers).  
Because these integrations sit outside our SLA envelope, we must define a **consistent retry and reconciliation strategy** to maintain correctness even when dependencies are slow, flaky, or unavailable.

### Questions

- How do we ensure at-least-once delivery without duplicate actions (e.g., duplicate unlocks or double charges)?  
- How do we recover gracefully from temporary upstream outages?  
- Where and how do we surface SLA breaches for monitoring and alerting?  
- How do we reconcile asynchronous state between MobilityCorp and external providers (e.g., payment captured but ride record failed to update)?

---

## Options

### Option 1 – Synchronous retries only (simple client-side retry)

Each service retries failed upstream calls a fixed number of times with exponential backoff.

**Pros**
- Simple to implement.  
- No new infrastructure.

**Cons**
- Retries block request threads.  
- No resilience to longer outages.  
- Difficult to guarantee idempotency or trace failed transactions.

---

### Option 2 – Dedicated **Outbox + Worker Queue** pattern (asynchronous delivery)

Each service writes external calls to an **outbox table** or message queue.  
A background worker processes the queue, ensuring idempotent delivery via unique command/request IDs.  
Failed deliveries are retried according to back-off policy; reconciliation jobs correct state divergence daily.

**Pros**
- Proven reliability pattern for distributed systems.  
- Supports exactly-once semantics when combined with idempotency keys.  
- Enables delayed retries and recovery from multi-hour outages.  
- Allows centralised SLA monitoring (success/failure metrics per dependency).

**Cons**
- Slight architectural complexity (queue infra, workers).
- Increased eventual-consistency latency.
- Requires schema and operational ownership per service.

---

### Option 3 – Central “Integration Gateway” service

Introduce a single gateway that manages all outbound requests to IoT, Payment, and IDV providers with a shared retry mechanism and monitoring dashboard.

**Pros**
- Uniform SLA policy and observability.  
- Simplifies downstream services.

**Cons**
- Creates a bottleneck and single point of failure.  
- Couples unrelated domains.  
- Higher coordination overhead.

---

## Decision

---

## Consequences


**Follow-ups**
- Define standard telemetry for external call metrics (success rate, latency, retry count).  
- Document expected SLA per provider (IoT ≥ 99.9%, Payments ≥ 99.99%, IDV ≥ 99.5%).  
- Include retry/back-off configuration in shared reliability library.  
- Capture final implementation in design spec once tooling is chosen (e.g., Kafka, SQS, or internal job queue).

---