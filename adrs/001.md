---
id: "0001"
title: External Dependency SLA & Retry Handling
status: proposed
date: 2025-10-17
---

## Context and Problem Statement

MobilityCorp is a ride sharing service providing on-demand vehicle access to users via a mobile app. Our platform orchestrates real-time ride requests, vehicle access, payments, and compliance checks, requiring seamless integration with multiple external systems outside our operational control:

- **IoT Integration API** – for real-time vehicle telemetry, remote lock/unlock commands, and status updates, enabling users to start and end rides autonomously.
- **Payment Provider** – for handling pre-authorisations, ride fare captures, and refunds, ensuring secure and reliable financial transactions.
- **Identity Verification (IDV / KYC)** – for validating user licences and age, supporting regulatory compliance and fraud prevention.

These dependencies are critical to MobilityCorp’s core user flows—starting rides, ending rides, and charging customers. Each introduces **latency, rate-limits, and failure modes** that can directly impact user experience, revenue, and trust. For example, a failed unlock command or delayed payment capture can result in lost rides, frustrated users, or financial discrepancies.

Because these integrations sit outside our SLA envelope, we must define a **consistent retry and reconciliation strategy** to maintain correctness, reliability, and a high-quality user experience even when dependencies are slow, flaky, or temporarily unavailable.

### Questions

- How do we ensure at-least-once delivery without duplicate actions (e.g., duplicate unlocks or double charges)?  
- How do we recover gracefully from temporary upstream outages?  
- Where and how do we surface SLA breaches for monitoring and alerting?  
- How do we reconcile asynchronous state between MobilityCorp and external providers (e.g., payment captured but ride record failed to update)?

---

## Options

### Option 1 – Synchronous retries only (simple client-side retry)

Each service retries failed upstream calls a fixed number of times with exponential backoff.

**Pros**
- Simple to implement.  
- No new infrastructure.

**Cons**
- Retries block request threads.  
- No resilience to longer outages.  
- Difficult to guarantee idempotency or trace failed transactions.

---

### Option 2 – Dedicated **Outbox + Worker Queue** pattern (asynchronous delivery)

Each service writes external calls to an **outbox table** or message queue.  
A background worker processes the queue, ensuring idempotent delivery via unique command/request IDs.  
Failed deliveries are retried according to back-off policy; reconciliation jobs correct state divergence daily.

**Pros**
- Proven reliability pattern for distributed systems.  
- Supports exactly-once semantics when combined with idempotency keys.  
- Enables delayed retries and recovery from multi-hour outages.  
- Allows centralised SLA monitoring (success/failure metrics per dependency).

**Cons**
- Slight architectural complexity (queue infra, workers).
- Increased eventual-consistency latency.
- Requires schema and operational ownership per service.

---

### Option 3 – Central “Integration Gateway” service

Introduce a single gateway that manages all outbound requests to IoT, Payment, and IDV providers with a shared retry mechanism and monitoring dashboard.

**Pros**
- Uniform SLA policy and observability.  
- Simplifies downstream services.

**Cons**
- Creates a bottleneck and single point of failure.  
- Couples unrelated domains.  
- Higher coordination overhead.

---

## Decision

We will adopt **Option 2 – Dedicated Outbox + Worker Queue pattern (asynchronous delivery)** for handling external dependency interactions.

**Rationale:**
- This pattern is proven in high-scale, distributed environments like ride sharing, where reliability and user experience are paramount.
- It provides robust reliability and supports at-least-once delivery with idempotency, which is critical for financial and operational correctness (e.g., avoiding double charges, duplicate unlocks, or missed ride completions).
- Enables recovery from both transient and prolonged outages, minimizing user disruption and lost revenue during provider downtime.
- Centralized monitoring and metrics collection will allow us to surface SLA breaches and operational issues promptly, supporting proactive incident response and customer support.
- Decouples user-facing flows from external system latency, improving app responsiveness and perceived reliability.
- While it introduces some architectural complexity, the benefits in resilience, observability, and correctness outweigh the costs, especially given the business-critical nature of these integrations.


Option 1 was rejected due to lack of resilience, inability to handle prolonged outages, and poor traceability—unacceptable for a real-time mobility platform. Option 3 was rejected due to the risk of creating a bottleneck, single point of failure, and coupling unrelated domains, which would hinder scalability and agility as the business grows.

---

## Consequences

**Positive Consequences:**
- Improved reliability and resilience to external system failures and outages, directly supporting MobilityCorp’s brand promise of always-available rides.
- Enables at-least-once delivery with idempotency, reducing risk of duplicate charges, missed unlocks, or lost ride records—protecting both user trust and company revenue.
- Centralized metrics and monitoring for SLA breaches and operational visibility, enabling rapid detection and resolution of issues that could impact ride availability or payment flows.
- Supports delayed retries and reconciliation, improving correctness in asynchronous scenarios and reducing manual intervention for support teams.
- Decouples user experience from external system latency, resulting in faster, more reliable app interactions for riders and drivers.

**Negative Consequences:**
- Increased architectural complexity (requires queue infrastructure, outbox tables, and background workers per service), which may lengthen onboarding for new engineers.
- Slightly higher operational overhead for maintaining and monitoring the queue and worker systems, including on-call responsibilities.
- Eventual consistency may introduce latency in some user flows (e.g., delayed payment capture or ride completion), requiring clear user messaging and support processes.

**Risks and Mitigations:**
- Risk: Outbox/queue system failure could block external calls, impacting ride starts/ends or payments. Mitigation: Use highly available managed queue services, implement robust monitoring, and establish clear incident response playbooks.
- Risk: Idempotency bugs could cause duplicate actions (e.g., double unlocks or charges). Mitigation: Enforce unique request/command IDs, comprehensive automated testing, and regular audits of reconciliation logic.

**Stakeholders:**
- Engineering, SRE, Product, and Customer Support teams must be informed and involved in implementation, monitoring, and incident response.

**Related ADRs:**
- See future ADRs for specific technology/tooling choices, reliability library design, and user experience messaging for asynchronous flows.

**Follow-ups**
- Define standard telemetry for external call metrics (success rate, latency, retry count).  
- Document expected SLA per provider (IoT ≥ 99.99%, Payments ≥ 99.99%, IDV ≥ 99.99%).  
- Include retry/back-off configuration in shared reliability library.  
- Capture final implementation in design spec once tooling is chosen (e.g., Kafka, SQS, or internal job queue).

---