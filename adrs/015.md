---
id: "0015"
title: Model Hosting & Inference Strategy
status: proposed
date: 2025-10-22
---

## Context and Problem Statement

MobilityCorp’s AI capabilities (forecasting, routing solver, anomaly detection, personalisation) depend on reliable and compliant **model hosting and inference infrastructure**.  
We want to ensure consistency in **latency, cost, observability**, and **data-residency**.

To achieve production-grade reliability, MobilityCorp must standardise how models are:

- **Deployed, scaled, and versioned** across environments.  
- **Served via managed inference endpoints** with predictable latency and cost.  
- Hosted exclusively within **EU regions** to ensure GDPR compliance.  
- Integrated with the **AI Interchange Layer (ADR-014)** for routing, cost tracking, and explainability.  

---

## Questions

- Should models be hosted on a managed ML platform (e.g., AWS SageMaker, Azure ML) or internally via Kubernetes?  
- How do we guarantee EU data residency and isolation?  
- What level of autoscaling and model-version control is required?  
- How do we measure and optimise inference cost per request?  
- How will deployments integrate with CI/CD and the AI Interchange governance model?  

---

## Options

### Option A — Self-Managed Inference on Kubernetes
Containerise and deploy models on existing Kubernetes clusters using custom APIs.

**Pros**
- Full control of runtime and scaling.  
- Reuses existing DevOps stack.  

**Cons**
- High operational overhead (autoscaling, GPU scheduling, patching).  
- Harder to guarantee data isolation and tenancy boundaries.  
- Lacks built-in model registry and monitoring.

---

### Option B — Third-Party Hosted Models (External APIs)
Rely on external AI APIs (e.g., OpenAI, Bedrock, Claude) for inference.

**Pros**
- Zero infrastructure maintenance.  
- Immediate scalability.  

**Cons**
- Data may leave EU jurisdiction.  
- Limited cost predictability.  
- Restricted observability and customisation.  

---

### Option C — Managed Cloud ML Platform (Recommended)
Use **AWS SageMaker (EU region)** or equivalent managed ML platform for hosting and inference.  
Integrate SageMaker endpoints with the **AI Interchange Layer (ADR-014)** and **Data Platform (ADR-007)** for monitoring, cost, and compliance.

**Pros**
- EU-resident managed environment with GDPR alignment.  
- Built-in model registry, versioning, and A/B deployment.  
- Predictable cost via instance-based pricing and autoscaling.  
- Integrates directly with existing AWS security, IAM, and monitoring.  
- Simplifies lifecycle management (train → register → deploy → monitor).  

**Cons**
- Slight vendor lock-in to AWS.  
- Requires well-defined IAM and VPC boundaries.  

---

## Recommendation

Adopt **Option C – Managed Cloud ML Platform (AWS SageMaker EU Region)** as the standard hosting and inference environment.

**Key decisions**
1. **Deployment Workflow:** Models trained in Databricks or offline pipelines are registered to SageMaker Model Registry.  
2. **Inference Endpoints:** Provisioned per model version with autoscaling and cost tags.  
3. **Integration:** The **AI Interchange Layer (ADR-014)** invokes SageMaker endpoints through secure VPC endpoints.  
4. **Data Residency:** All endpoints, logs, and metrics restricted to EU regions.  
5. **Observability:** OpenTelemetry exporters and CloudWatch metrics feed into ADR-013 dashboards.  
6. **Cost Tracking:** Instance hours, invocations, and data transfer logged to Databricks and Audit Log (ADR-011).  
7. **Failover:** Low-traffic fallback endpoints can be provisioned for redundancy.

---

## Consequences

**Positive**
- Predictable cost, scalable inference, and clear compliance posture.  
- Standardised deployment pipeline across all AI services.  
- Simplified lifecycle (train → deploy → monitor) via managed platform.  

**Trade-offs**
- Some dependency on AWS infrastructure and pricing model.  
- Requires IAM guardrails and monitoring for cost caps.  
- Learning curve for data science teams new to SageMaker workflows.  

---

## Implementation Details (High-level)

| Component | Responsibility | Notes |
|---|---|---|
| **Databricks / Training Pipelines** | Train and export model artifacts | Uses EU-resident storage |
| **Model Registry (SageMaker)** | Store model metadata, lineage, versions | Integrated with CI/CD |
| **SageMaker Endpoints** | Host inference APIs with autoscaling | Private VPC, EU region |
| **AI Interchange Layer (ADR-014)** | Route inference requests, collect cost metrics | Centralised governance |
| **Kafka / Event Bus** | Emit inference events and usage metrics | Shared ingestion layer |
| **Databricks Lakehouse** | Store cost, latency, and performance data | Supports analytics and SLO tracking |
| **Audit Log (ADR-011)** | Immutable record of deployments and cost | Verifiable chain |
| **Observability Stack (ADR-013)** | Collect logs, traces, metrics from endpoints | Unified schema and dashboards |

**Sequence Example**

<img width="4046" height="1074" alt="15" src="https://github.com/user-attachments/assets/e0fea51f-117f-474b-b53d-5f9a6dfed428" />

---

## Risks and Mitigations

| Risk                                         | Likelihood | Impact | Mitigation                                     |
| -------------------------------------------- | ---------: | -----: | ---------------------------------------------- |
| AWS service outage in EU region              |        Low |   High | Multi-AZ deployment, cross-region backup       |
| Unexpected inference cost spikes             |        Med |    Med | Quotas, budget alerts, auto-scale limits       |
| Model or data leakage                        |        Low |   High | VPC isolation, encryption, IAM least privilege |
| Vendor lock-in                               |        Med |    Med | Abstract through AI Interchange Layer          |
| Drift in model performance                   |        Med |    Med | Scheduled re-training and monitoring           |
| Compliance breach (data transfer outside EU) |        Low |   High | Region restriction and periodic audits         |

---

## Alternatives Considered

* Self-managed Kubernetes inference – rejected for operational overhead.
* External third-party AI APIs – rejected for compliance and observability issues.
* Managed cloud inference – chosen for compliance, cost predictability, and integration with AI Interchange.

## Links

* ADR-007 – Data Platform Architecture
* ADR-011 – Immutable Audit Log Architecture
* ADR-013 – Observability & Metrics Standardisation
* ADR-014 – AI Interchange Layer Architecture
* ADR-005 – Routing Solver (VRP-TW)
* ADR-009 – Personalisation / Concierge Service Architecture